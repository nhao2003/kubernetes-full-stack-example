
==> Audit <==
|--------------|----------------|----------|----------------|---------|---------------------|---------------------|
|   Command    |      Args      | Profile  |      User      | Version |     Start Time      |      End Time       |
|--------------|----------------|----------|----------------|---------|---------------------|---------------------|
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 14:37 +07 |                     |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 14:40 +07 |                     |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 14:42 +07 |                     |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 14:49 +07 | 05 Jan 25 14:51 +07 |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 14:52 +07 | 05 Jan 25 14:53 +07 |
| dashboard    |                | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 14:56 +07 |                     |
| service      | service1 --url | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 15:37 +07 | 05 Jan 25 15:37 +07 |
| service      | service1 --url | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 15:41 +07 |                     |
| service      | service3 --url | minikube | NHAO2003\nhath | v1.34.0 | 05 Jan 25 17:14 +07 |                     |
| update-check |                | minikube | NHAO2003\nhath | v1.34.0 | 09 Jan 25 23:03 +07 | 09 Jan 25 23:03 +07 |
| update-check |                | minikube | NHAO2003\nhath | v1.34.0 | 09 Jan 25 23:06 +07 | 09 Jan 25 23:06 +07 |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 09 Jan 25 23:10 +07 | 09 Jan 25 23:12 +07 |
| addons       | enable ingress | minikube | NHAO2003\nhath | v1.34.0 | 09 Jan 25 23:15 +07 |                     |
| ip           |                | minikube | NHAO2003\nhath | v1.34.0 | 09 Jan 25 23:22 +07 | 09 Jan 25 23:22 +07 |
| update-check |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 21:40 +07 |                     |
| update-check |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 21:41 +07 | 10 Jan 25 21:41 +07 |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 21:41 +07 |                     |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 21:43 +07 |                     |
| delete       |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 21:48 +07 | 10 Jan 25 21:48 +07 |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 21:48 +07 | 10 Jan 25 21:50 +07 |
| update-check |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 22:09 +07 | 10 Jan 25 22:09 +07 |
| delete       |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 22:10 +07 | 10 Jan 25 22:10 +07 |
| start        |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 22:10 +07 | 10 Jan 25 22:12 +07 |
| addons       | enable ingress | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 22:15 +07 |                     |
| ip           |                | minikube | NHAO2003\nhath | v1.34.0 | 10 Jan 25 22:19 +07 | 10 Jan 25 22:19 +07 |
|--------------|----------------|----------|----------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/01/10 22:10:43
Running on machine: nhao2003
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0110 22:10:43.146284   23180 out.go:345] Setting OutFile to fd 112 ...
I0110 22:10:43.146808   23180 out.go:397] isatty.IsTerminal(112) = true
I0110 22:10:43.146808   23180 out.go:358] Setting ErrFile to fd 116...
I0110 22:10:43.147332   23180 out.go:397] isatty.IsTerminal(116) = true
I0110 22:10:43.183468   23180 out.go:352] Setting JSON to false
I0110 22:10:43.190339   23180 start.go:129] hostinfo: {"hostname":"nhao2003","uptime":176426,"bootTime":1736345416,"procs":290,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.2605 Build 26100.2605","kernelVersion":"10.0.26100.2605 Build 26100.2605","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"e46d3884-b1cb-4456-87b2-7d5bc6d620a2"}
W0110 22:10:43.190339   23180 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0110 22:10:43.195540   23180 out.go:177] 😄  minikube v1.34.0 on Microsoft Windows 11 Pro 10.0.26100.2605 Build 26100.2605
I0110 22:10:43.203171   23180 notify.go:220] Checking for updates...
I0110 22:10:43.204197   23180 driver.go:394] Setting default libvirt URI to qemu:///system
I0110 22:10:43.204197   23180 global.go:112] Querying for installed drivers using PATH=C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\Microsoft SQL Server\150\Tools\Binn\;C:\Program Files\Microsoft SQL Server\Client SDK\ODBC\170\Tools\Binn\;C:\Program Files\dotnet\;C:\Program Files\Git\cmd;C:\Program Files\Docker\Docker\resources\bin;C:\Program Files\nodejs\;C:\Dev\flutter\bin;C:\Program Files (x86)\Windows Kits\10\Windows Performance Toolkit\;C:\Program Files\Cloudflare\Cloudflare WARP\;C:\Program Files\Kubernetes\Minikube;C:\Users\nhath\AppData\Local\Programs\Python\Python313\Scripts\;C:\Users\nhath\AppData\Local\Programs\Python\Python313\;C:\Users\nhath\.cargo\bin;C:\Users\nhath\AppData\Local\Microsoft\WindowsApps;C:\Users\nhath\AppData\Local\JetBrains\JetBrains Rider 2024.3\bin;;C:\Users\nhath\AppData\Local\Programs\Microsoft VS Code\bin;C:\Users\nhath\AppData\Roaming\npm;C:\Users\nhath\.bun\bin;C:\Users\nhath\AppData\Local\Programs\Ollama;C:\Users\nhath\AppData\Roaming\Python\Scripts;C:\Users\nhath\.dotnet\tools;C:\Users\nhath\AppData\Roaming\Python\Scripts;APPDATA%\pypoetry\venv\Scripts\poetry;C:\Users\nhath\AppData\Local\GitHubDesktop\bin;C:\Users\nhath\git-redate;;c:\Users\nhath\AppData\Roaming\Code\User\globalStorage\github.copilot-chat\debugCommand
I0110 22:10:43.220304   23180 global.go:133] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in %PATH% Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0110 22:10:43.220945   23180 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0110 22:10:44.468373   23180 global.go:133] hyperv default: true priority: 8, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:Hyper-V requires Administrator privileges Reason: Fix:Right-click the PowerShell icon and select Run as Administrator to open PowerShell in elevated mode. Doc: Version:}
I0110 22:10:44.482190   23180 global.go:133] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in %PATH% Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0110 22:10:44.508930   23180 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0110 22:10:44.522116   23180 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in %PATH% Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0110 22:10:44.617230   23180 docker.go:123] docker version: linux-27.3.1:Docker Desktop 4.36.0 (175267)
I0110 22:10:44.622143   23180 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0110 22:10:44.918905   23180 info.go:266] docker info: {ID:a1a8336b-80a8-47fa-93e3-fcddb3d2ee1d Containers:10 ContainersRunning:0 ContainersPaused:0 ContainersStopped:10 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:81 SystemTime:2025-01-10 15:10:45.212087969 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:12194070528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\nhath\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.1.0] map[Name:buildx Path:C:\Users\nhath\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.18.0-desktop.2] map[Name:compose Path:C:\Users\nhath\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.30.3-desktop.1] map[Name:debug Path:C:\Users\nhath\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Users\nhath\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Users\nhath\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\nhath\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\nhath\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\nhath\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\nhath\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\nhath\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.0]] Warnings:<nil>}}
I0110 22:10:44.919445   23180 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0110 22:10:44.919445   23180 driver.go:316] not recommending "ssh" due to default: false
I0110 22:10:44.919445   23180 driver.go:311] not recommending "hyperv" due to health: Hyper-V requires Administrator privileges
I0110 22:10:44.919445   23180 driver.go:351] Picked: docker
I0110 22:10:44.919445   23180 driver.go:352] Alternatives: [ssh]
I0110 22:10:44.919445   23180 driver.go:353] Rejects: [podman hyperv qemu2 virtualbox vmware]
I0110 22:10:44.921118   23180 out.go:177] ✨  Automatically selected the docker driver
I0110 22:10:44.923377   23180 start.go:297] selected driver: docker
I0110 22:10:44.923377   23180 start.go:901] validating driver "docker" against <nil>
I0110 22:10:44.923377   23180 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0110 22:10:44.933759   23180 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0110 22:10:45.226448   23180 info.go:266] docker info: {ID:a1a8336b-80a8-47fa-93e3-fcddb3d2ee1d Containers:10 ContainersRunning:0 ContainersPaused:0 ContainersStopped:10 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:53 OomKillDisable:true NGoroutines:81 SystemTime:2025-01-10 15:10:45.535537379 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:12194070528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\nhath\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.1.0] map[Name:buildx Path:C:\Users\nhath\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.18.0-desktop.2] map[Name:compose Path:C:\Users\nhath\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.30.3-desktop.1] map[Name:debug Path:C:\Users\nhath\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Users\nhath\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Users\nhath\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\nhath\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\nhath\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\nhath\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\nhath\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\nhath\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.0]] Warnings:<nil>}}
I0110 22:10:45.226986   23180 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0110 22:10:45.291769   23180 start_flags.go:393] Using suggested 5900MB memory alloc based on sys=23898MB, container=11629MB
I0110 22:10:45.292285   23180 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0110 22:10:45.293428   23180 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0110 22:10:45.295103   23180 cni.go:84] Creating CNI manager for ""
I0110 22:10:45.295103   23180 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0110 22:10:45.295103   23180 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0110 22:10:45.295103   23180 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\nhath:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0110 22:10:45.296804   23180 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0110 22:10:45.297921   23180 cache.go:121] Beginning downloading kic base image for docker with docker
I0110 22:10:45.299574   23180 out.go:177] 🚜  Pulling base image v0.0.45 ...
I0110 22:10:45.301223   23180 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0110 22:10:45.301223   23180 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I0110 22:10:45.301765   23180 preload.go:146] Found local preload: C:\Users\nhath\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I0110 22:10:45.301765   23180 cache.go:56] Caching tarball of preloaded images
I0110 22:10:45.301765   23180 preload.go:172] Found C:\Users\nhath\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0110 22:10:45.302315   23180 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I0110 22:10:45.302861   23180 profile.go:143] Saving config to C:\Users\nhath\.minikube\profiles\minikube\config.json ...
I0110 22:10:45.303424   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.minikube\profiles\minikube\config.json: {Name:mk134da7badf9f60edc0ac9ac3325576995b7ba6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:10:45.634227   23180 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0110 22:10:45.634227   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0110 22:10:45.634794   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0110 22:10:45.634794   23180 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0110 22:10:45.634794   23180 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0110 22:10:45.634794   23180 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0110 22:10:45.634794   23180 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0110 22:10:45.634794   23180 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0110 22:10:45.634794   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0110 22:10:45.635356   23180 cache.go:168] failed to download gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85, will try fallback image if available: tarball: unexpected EOF
I0110 22:10:45.635356   23180 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
W0110 22:10:46.032668   23180 image.go:95] image docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I0110 22:10:46.033327   23180 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I0110 22:10:46.033327   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0110 22:10:46.033327   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0110 22:10:46.033900   23180 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I0110 22:10:46.033900   23180 image.go:66] Found docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I0110 22:10:46.033900   23180 image.go:135] docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I0110 22:10:46.033900   23180 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I0110 22:10:46.033900   23180 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I0110 22:10:46.033900   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\stable_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I0110 22:10:46.033900   23180 cache.go:168] failed to download docker.io/kicbase/stable:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85, will try fallback image if available: tarball: unexpected EOF
I0110 22:10:46.033900   23180 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local docker daemon
I0110 22:10:46.334383   23180 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45 to local cache
I0110 22:10:46.334942   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0110 22:10:46.334942   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0110 22:10:46.334942   23180 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory
I0110 22:10:46.334942   23180 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45 in local cache directory, skipping pull
I0110 22:10:46.334942   23180 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45 exists in cache, skipping pull
I0110 22:10:46.335494   23180 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45 as a tarball
I0110 22:10:46.335494   23180 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45 from local cache
I0110 22:10:46.335494   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\kicbase:v0.0.45.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\kicbase_v0.0.45.tar
I0110 22:10:46.335494   23180 cache.go:168] failed to download gcr.io/k8s-minikube/kicbase:v0.0.45, will try fallback image if available: tarball: unexpected EOF
I0110 22:10:46.335494   23180 image.go:79] Checking for docker.io/kicbase/stable:v0.0.45 in local docker daemon
W0110 22:10:46.747939   23180 image.go:95] image docker.io/kicbase/stable:v0.0.45 is of wrong architecture
I0110 22:10:46.747939   23180 cache.go:149] Downloading docker.io/kicbase/stable:v0.0.45 to local cache
I0110 22:10:46.748552   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\stable:v0.0.45.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\stable_v0.0.45.tar
I0110 22:10:46.748552   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\stable:v0.0.45.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\stable_v0.0.45.tar
I0110 22:10:46.748552   23180 image.go:63] Checking for docker.io/kicbase/stable:v0.0.45 in local cache directory
I0110 22:10:46.748552   23180 image.go:66] Found docker.io/kicbase/stable:v0.0.45 in local cache directory, skipping pull
I0110 22:10:46.748552   23180 image.go:135] docker.io/kicbase/stable:v0.0.45 exists in cache, skipping pull
I0110 22:10:46.748552   23180 cache.go:152] successfully saved docker.io/kicbase/stable:v0.0.45 as a tarball
I0110 22:10:46.748552   23180 cache.go:162] Loading docker.io/kicbase/stable:v0.0.45 from local cache
I0110 22:10:46.749126   23180 localpath.go:151] windows sanitize: C:\Users\nhath\.minikube\cache\kic\amd64\stable:v0.0.45.tar -> C:\Users\nhath\.minikube\cache\kic\amd64\stable_v0.0.45.tar
I0110 22:11:27.893149   23180 cache.go:164] successfully loaded and using docker.io/kicbase/stable:v0.0.45 from cached tarball
W0110 22:11:27.893911   23180 out.go:270] ❗  minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.45, but successfully downloaded docker.io/kicbase/stable:v0.0.45 as a fallback image
I0110 22:11:27.894462   23180 cache.go:194] Successfully downloaded all kic artifacts
I0110 22:11:27.895047   23180 start.go:360] acquireMachinesLock for minikube: {Name:mkd65d15ea47872defc2341b6e0f84e6d002846c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0110 22:11:27.895047   23180 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0110 22:11:27.895619   23180 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\nhath:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0110 22:11:27.895619   23180 start.go:125] createHost starting for "" (driver="docker")
I0110 22:11:27.898748   23180 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=5900MB) ...
I0110 22:11:27.900489   23180 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0110 22:11:27.900489   23180 client.go:168] LocalClient.Create starting
I0110 22:11:27.901092   23180 main.go:141] libmachine: Reading certificate data from C:\Users\nhath\.minikube\certs\ca.pem
I0110 22:11:27.901694   23180 main.go:141] libmachine: Decoding PEM data...
I0110 22:11:27.902280   23180 main.go:141] libmachine: Parsing certificate...
I0110 22:11:27.902280   23180 main.go:141] libmachine: Reading certificate data from C:\Users\nhath\.minikube\certs\cert.pem
I0110 22:11:27.902839   23180 main.go:141] libmachine: Decoding PEM data...
I0110 22:11:27.902839   23180 main.go:141] libmachine: Parsing certificate...
I0110 22:11:27.922663   23180 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0110 22:11:28.075751   23180 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0110 22:11:28.084182   23180 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0110 22:11:28.084182   23180 cli_runner.go:164] Run: docker network inspect minikube
W0110 22:11:28.193607   23180 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0110 22:11:28.193607   23180 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0110 22:11:28.193607   23180 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0110 22:11:28.200770   23180 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0110 22:11:28.363417   23180 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001e93170}
I0110 22:11:28.363417   23180 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0110 22:11:28.368537   23180 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0110 22:11:28.551673   23180 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0110 22:11:28.551673   23180 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0110 22:11:28.564030   23180 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0110 22:11:28.891738   23180 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0110 22:11:28.988253   23180 oci.go:103] Successfully created a docker volume minikube
I0110 22:11:28.995714   23180 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.45 -d /var/lib
I0110 22:11:30.547055   23180 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var docker.io/kicbase/stable:v0.0.45 -d /var/lib: (1.5513402s)
I0110 22:11:30.547647   23180 oci.go:107] Successfully prepared a docker volume minikube
I0110 22:11:30.547647   23180 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0110 22:11:30.547647   23180 kic.go:194] Starting extracting preloaded images to volume ...
I0110 22:11:30.554064   23180 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\nhath\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.45 -I lz4 -xf /preloaded.tar -C /extractDir
I0110 22:11:44.680258   23180 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\nhath\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir docker.io/kicbase/stable:v0.0.45 -I lz4 -xf /preloaded.tar -C /extractDir: (14.1256289s)
I0110 22:11:44.680258   23180 kic.go:203] duration metric: took 14.1326113s to extract preloaded images to volume ...
I0110 22:11:44.685502   23180 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0110 22:11:45.109017   23180 info.go:266] docker info: {ID:a1a8336b-80a8-47fa-93e3-fcddb3d2ee1d Containers:10 ContainersRunning:0 ContainersPaused:0 ContainersStopped:10 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:true NGoroutines:83 SystemTime:2025-01-10 15:11:45.525867485 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:12194070528 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\nhath\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.1.0] map[Name:buildx Path:C:\Users\nhath\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.18.0-desktop.2] map[Name:compose Path:C:\Users\nhath\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.30.3-desktop.1] map[Name:debug Path:C:\Users\nhath\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.37] map[Name:desktop Path:C:\Users\nhath\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.15] map[Name:dev Path:C:\Users\nhath\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\nhath\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\nhath\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\nhath\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\nhath\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\nhath\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.15.0]] Warnings:<nil>}}
I0110 22:11:45.114380   23180 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0110 22:11:45.422676   23180 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=5900mb --memory-swap=5900mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 docker.io/kicbase/stable:v0.0.45
I0110 22:11:46.279312   23180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0110 22:11:46.385994   23180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 22:11:46.482048   23180 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0110 22:11:46.645574   23180 oci.go:144] the created container "minikube" has a running status.
I0110 22:11:46.645574   23180 kic.go:225] Creating ssh key for kic: C:\Users\nhath\.minikube\machines\minikube\id_rsa...
I0110 22:11:47.017596   23180 kic_runner.go:191] docker (temp): C:\Users\nhath\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0110 22:11:47.169811   23180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 22:11:47.304028   23180 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0110 22:11:47.304028   23180 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0110 22:11:47.557429   23180 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\nhath\.minikube\machines\minikube\id_rsa...
I0110 22:11:48.344231   23180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 22:11:48.430682   23180 machine.go:93] provisionDockerMachine start ...
I0110 22:11:48.435660   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:48.522779   23180 main.go:141] libmachine: Using SSH client type: native
I0110 22:11:48.537877   23180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10cc9c0] 0x10cf5a0 <nil>  [] 0s} 127.0.0.1 53375 <nil> <nil>}
I0110 22:11:48.537877   23180 main.go:141] libmachine: About to run SSH command:
hostname
I0110 22:11:48.719530   23180 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0110 22:11:48.719530   23180 ubuntu.go:169] provisioning hostname "minikube"
I0110 22:11:48.724686   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:48.806389   23180 main.go:141] libmachine: Using SSH client type: native
I0110 22:11:48.806945   23180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10cc9c0] 0x10cf5a0 <nil>  [] 0s} 127.0.0.1 53375 <nil> <nil>}
I0110 22:11:48.806945   23180 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0110 22:11:49.010684   23180 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0110 22:11:49.015054   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:49.094074   23180 main.go:141] libmachine: Using SSH client type: native
I0110 22:11:49.094641   23180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10cc9c0] 0x10cf5a0 <nil>  [] 0s} 127.0.0.1 53375 <nil> <nil>}
I0110 22:11:49.094641   23180 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0110 22:11:49.270455   23180 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0110 22:11:49.270455   23180 ubuntu.go:175] set auth options {CertDir:C:\Users\nhath\.minikube CaCertPath:C:\Users\nhath\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\nhath\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\nhath\.minikube\machines\server.pem ServerKeyPath:C:\Users\nhath\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\nhath\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\nhath\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\nhath\.minikube}
I0110 22:11:49.270455   23180 ubuntu.go:177] setting up certificates
I0110 22:11:49.270455   23180 provision.go:84] configureAuth start
I0110 22:11:49.275561   23180 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0110 22:11:49.354802   23180 provision.go:143] copyHostCerts
I0110 22:11:49.355362   23180 exec_runner.go:144] found C:\Users\nhath\.minikube/ca.pem, removing ...
I0110 22:11:49.355362   23180 exec_runner.go:203] rm: C:\Users\nhath\.minikube\ca.pem
I0110 22:11:49.355362   23180 exec_runner.go:151] cp: C:\Users\nhath\.minikube\certs\ca.pem --> C:\Users\nhath\.minikube/ca.pem (1074 bytes)
I0110 22:11:49.356461   23180 exec_runner.go:144] found C:\Users\nhath\.minikube/cert.pem, removing ...
I0110 22:11:49.356461   23180 exec_runner.go:203] rm: C:\Users\nhath\.minikube\cert.pem
I0110 22:11:49.356999   23180 exec_runner.go:151] cp: C:\Users\nhath\.minikube\certs\cert.pem --> C:\Users\nhath\.minikube/cert.pem (1119 bytes)
I0110 22:11:49.358088   23180 exec_runner.go:144] found C:\Users\nhath\.minikube/key.pem, removing ...
I0110 22:11:49.358088   23180 exec_runner.go:203] rm: C:\Users\nhath\.minikube\key.pem
I0110 22:11:49.358088   23180 exec_runner.go:151] cp: C:\Users\nhath\.minikube\certs\key.pem --> C:\Users\nhath\.minikube/key.pem (1675 bytes)
I0110 22:11:49.359188   23180 provision.go:117] generating server cert: C:\Users\nhath\.minikube\machines\server.pem ca-key=C:\Users\nhath\.minikube\certs\ca.pem private-key=C:\Users\nhath\.minikube\certs\ca-key.pem org=nhath.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0110 22:11:49.637455   23180 provision.go:177] copyRemoteCerts
I0110 22:11:49.638441   23180 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0110 22:11:49.641980   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:49.721072   23180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53375 SSHKeyPath:C:\Users\nhath\.minikube\machines\minikube\id_rsa Username:docker}
I0110 22:11:49.859507   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0110 22:11:49.912961   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0110 22:11:49.968508   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0110 22:11:50.020977   23180 provision.go:87] duration metric: took 750.5218ms to configureAuth
I0110 22:11:50.020977   23180 ubuntu.go:193] setting minikube options for container-runtime
I0110 22:11:50.022641   23180 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0110 22:11:50.027132   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:50.106731   23180 main.go:141] libmachine: Using SSH client type: native
I0110 22:11:50.107275   23180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10cc9c0] 0x10cf5a0 <nil>  [] 0s} 127.0.0.1 53375 <nil> <nil>}
I0110 22:11:50.107275   23180 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0110 22:11:50.287861   23180 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0110 22:11:50.287861   23180 ubuntu.go:71] root file system type: overlay
I0110 22:11:50.287861   23180 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0110 22:11:50.292947   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:50.372040   23180 main.go:141] libmachine: Using SSH client type: native
I0110 22:11:50.372040   23180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10cc9c0] 0x10cf5a0 <nil>  [] 0s} 127.0.0.1 53375 <nil> <nil>}
I0110 22:11:50.372604   23180 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0110 22:11:50.579327   23180 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0110 22:11:50.584392   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:50.665874   23180 main.go:141] libmachine: Using SSH client type: native
I0110 22:11:50.666448   23180 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x10cc9c0] 0x10cf5a0 <nil>  [] 0s} 127.0.0.1 53375 <nil> <nil>}
I0110 22:11:50.666448   23180 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0110 22:11:52.289350   23180 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-08-27 14:13:43.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-01-10 15:11:49.782546050 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0110 22:11:52.289350   23180 machine.go:96] duration metric: took 3.8586675s to provisionDockerMachine
I0110 22:11:52.289350   23180 client.go:171] duration metric: took 24.3888604s to LocalClient.Create
I0110 22:11:52.289350   23180 start.go:167] duration metric: took 24.3894023s to libmachine.API.Create "minikube"
I0110 22:11:52.289350   23180 start.go:293] postStartSetup for "minikube" (driver="docker")
I0110 22:11:52.289350   23180 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0110 22:11:52.291702   23180 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0110 22:11:52.298694   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:52.386793   23180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53375 SSHKeyPath:C:\Users\nhath\.minikube\machines\minikube\id_rsa Username:docker}
I0110 22:11:52.548819   23180 ssh_runner.go:195] Run: cat /etc/os-release
I0110 22:11:52.559671   23180 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0110 22:11:52.559671   23180 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0110 22:11:52.559671   23180 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0110 22:11:52.559671   23180 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0110 22:11:52.559671   23180 filesync.go:126] Scanning C:\Users\nhath\.minikube\addons for local assets ...
I0110 22:11:52.560215   23180 filesync.go:126] Scanning C:\Users\nhath\.minikube\files for local assets ...
I0110 22:11:52.560215   23180 start.go:296] duration metric: took 270.865ms for postStartSetup
I0110 22:11:52.567478   23180 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0110 22:11:52.647975   23180 profile.go:143] Saving config to C:\Users\nhath\.minikube\profiles\minikube\config.json ...
I0110 22:11:52.669181   23180 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0110 22:11:52.673739   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:52.753651   23180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53375 SSHKeyPath:C:\Users\nhath\.minikube\machines\minikube\id_rsa Username:docker}
I0110 22:11:52.889471   23180 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0110 22:11:52.902115   23180 start.go:128] duration metric: took 25.0064963s to createHost
I0110 22:11:52.902115   23180 start.go:83] releasing machines lock for "minikube", held for 25.0070684s
I0110 22:11:52.907229   23180 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0110 22:11:52.984454   23180 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0110 22:11:52.988862   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:52.998240   23180 ssh_runner.go:195] Run: cat /version.json
I0110 22:11:53.002821   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:11:53.068491   23180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53375 SSHKeyPath:C:\Users\nhath\.minikube\machines\minikube\id_rsa Username:docker}
I0110 22:11:53.086340   23180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53375 SSHKeyPath:C:\Users\nhath\.minikube\machines\minikube\id_rsa Username:docker}
W0110 22:11:53.183302   23180 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0110 22:11:53.229195   23180 ssh_runner.go:195] Run: systemctl --version
I0110 22:11:53.254346   23180 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0110 22:11:53.267526   23180 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0110 22:11:53.290292   23180 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0110 22:11:53.291439   23180 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0110 22:11:53.353570   23180 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0110 22:11:53.353570   23180 start.go:495] detecting cgroup driver to use...
I0110 22:11:53.353570   23180 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0110 22:11:53.354088   23180 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 22:11:53.404108   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
W0110 22:11:53.439721   23180 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0110 22:11:53.440850   23180 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0110 22:11:53.444331   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0110 22:11:53.467737   23180 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0110 22:11:53.483056   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0110 22:11:53.521452   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 22:11:53.560096   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0110 22:11:53.598943   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 22:11:53.635687   23180 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0110 22:11:53.669855   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0110 22:11:53.709877   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0110 22:11:53.746018   23180 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0110 22:11:53.769971   23180 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0110 22:11:53.791171   23180 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0110 22:11:53.812541   23180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 22:11:53.982204   23180 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0110 22:11:54.186036   23180 start.go:495] detecting cgroup driver to use...
I0110 22:11:54.186036   23180 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0110 22:11:54.187200   23180 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0110 22:11:54.214092   23180 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0110 22:11:54.214661   23180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0110 22:11:54.240995   23180 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 22:11:54.294479   23180 ssh_runner.go:195] Run: which cri-dockerd
I0110 22:11:54.305364   23180 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0110 22:11:54.328412   23180 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0110 22:11:54.368447   23180 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0110 22:11:54.577582   23180 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0110 22:11:54.771114   23180 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0110 22:11:54.771114   23180 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0110 22:11:54.808537   23180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 22:11:54.985575   23180 ssh_runner.go:195] Run: sudo systemctl restart docker
I0110 22:11:55.791242   23180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0110 22:11:55.818709   23180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0110 22:11:55.846459   23180 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0110 22:11:56.018285   23180 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 22:11:56.191751   23180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 22:11:56.360822   23180 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0110 22:11:56.391969   23180 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0110 22:11:56.417435   23180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 22:11:56.578267   23180 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0110 22:11:56.757396   23180 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0110 22:11:56.773768   23180 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0110 22:11:56.783854   23180 start.go:563] Will wait 60s for crictl version
I0110 22:11:56.797865   23180 ssh_runner.go:195] Run: which crictl
I0110 22:11:56.808657   23180 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0110 22:11:56.917381   23180 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I0110 22:11:56.922570   23180 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 22:11:56.998158   23180 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 22:11:57.052816   23180 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I0110 22:11:57.057643   23180 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0110 22:11:57.246168   23180 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0110 22:11:57.266696   23180 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0110 22:11:57.276791   23180 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 22:11:57.311006   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0110 22:11:57.408112   23180 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\nhath:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0110 22:11:57.408112   23180 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I0110 22:11:57.413728   23180 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0110 22:11:57.461372   23180 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0110 22:11:57.461372   23180 docker.go:615] Images already preloaded, skipping extraction
I0110 22:11:57.467036   23180 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0110 22:11:57.510615   23180 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0110 22:11:57.510615   23180 cache_images.go:84] Images are preloaded, skipping loading
I0110 22:11:57.510615   23180 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I0110 22:11:57.510615   23180 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0110 22:11:57.516979   23180 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0110 22:11:57.663375   23180 cni.go:84] Creating CNI manager for ""
I0110 22:11:57.663375   23180 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0110 22:11:57.663375   23180 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0110 22:11:57.663375   23180 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0110 22:11:57.663936   23180 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0110 22:11:57.665102   23180 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I0110 22:11:57.686082   23180 binaries.go:44] Found k8s binaries, skipping transfer
I0110 22:11:57.686644   23180 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0110 22:11:57.709296   23180 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0110 22:11:57.749410   23180 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0110 22:11:57.788393   23180 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0110 22:11:57.843021   23180 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0110 22:11:57.854205   23180 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 22:11:57.881762   23180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 22:11:58.056189   23180 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0110 22:11:58.086823   23180 certs.go:68] Setting up C:\Users\nhath\.minikube\profiles\minikube for IP: 192.168.49.2
I0110 22:11:58.086823   23180 certs.go:194] generating shared ca certs ...
I0110 22:11:58.086823   23180 certs.go:226] acquiring lock for ca certs: {Name:mke0fd1d6293f38582edbf3391e1cefc7bfa2bc4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:11:58.087403   23180 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\nhath\.minikube\ca.key
I0110 22:11:58.087988   23180 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\nhath\.minikube\proxy-client-ca.key
I0110 22:11:58.087988   23180 certs.go:256] generating profile certs ...
I0110 22:11:58.088563   23180 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\nhath\.minikube\profiles\minikube\client.key
I0110 22:11:58.088563   23180 crypto.go:68] Generating cert C:\Users\nhath\.minikube\profiles\minikube\client.crt with IP's: []
I0110 22:11:58.873853   23180 crypto.go:156] Writing cert to C:\Users\nhath\.minikube\profiles\minikube\client.crt ...
I0110 22:11:58.873853   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.minikube\profiles\minikube\client.crt: {Name:mk03f0b8af8dd628420d98153f85299542d313f1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:11:58.875867   23180 crypto.go:164] Writing key to C:\Users\nhath\.minikube\profiles\minikube\client.key ...
I0110 22:11:58.875867   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.minikube\profiles\minikube\client.key: {Name:mkdfbee08d83d40184b89c77ef7c8349f074c747 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:11:58.876872   23180 certs.go:363] generating signed profile cert for "minikube": C:\Users\nhath\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0110 22:11:58.876872   23180 crypto.go:68] Generating cert C:\Users\nhath\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0110 22:11:59.452408   23180 crypto.go:156] Writing cert to C:\Users\nhath\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0110 22:11:59.452408   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkc8054fdbdbf3729b6062dc056e66214ad07fb1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:11:59.454433   23180 crypto.go:164] Writing key to C:\Users\nhath\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0110 22:11:59.454433   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mkdaa32f13560b644bb6fd2646566ba1002db75f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:11:59.456132   23180 certs.go:381] copying C:\Users\nhath\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\nhath\.minikube\profiles\minikube\apiserver.crt
I0110 22:11:59.475552   23180 certs.go:385] copying C:\Users\nhath\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\nhath\.minikube\profiles\minikube\apiserver.key
I0110 22:11:59.477413   23180 certs.go:363] generating signed profile cert for "aggregator": C:\Users\nhath\.minikube\profiles\minikube\proxy-client.key
I0110 22:11:59.477413   23180 crypto.go:68] Generating cert C:\Users\nhath\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0110 22:11:59.789294   23180 crypto.go:156] Writing cert to C:\Users\nhath\.minikube\profiles\minikube\proxy-client.crt ...
I0110 22:11:59.789294   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.minikube\profiles\minikube\proxy-client.crt: {Name:mkc08ef6428597449b5d10a30a0eb46dac2fde06 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:11:59.791306   23180 crypto.go:164] Writing key to C:\Users\nhath\.minikube\profiles\minikube\proxy-client.key ...
I0110 22:11:59.791306   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.minikube\profiles\minikube\proxy-client.key: {Name:mk99b917076dc712adda10a24f0a5389636c5fe9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:11:59.808107   23180 certs.go:484] found cert: C:\Users\nhath\.minikube\certs\ca-key.pem (1679 bytes)
I0110 22:11:59.808107   23180 certs.go:484] found cert: C:\Users\nhath\.minikube\certs\ca.pem (1074 bytes)
I0110 22:11:59.808107   23180 certs.go:484] found cert: C:\Users\nhath\.minikube\certs\cert.pem (1119 bytes)
I0110 22:11:59.809102   23180 certs.go:484] found cert: C:\Users\nhath\.minikube\certs\key.pem (1675 bytes)
I0110 22:11:59.812622   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0110 22:11:59.872662   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0110 22:11:59.930980   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0110 22:12:00.008433   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0110 22:12:00.071952   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0110 22:12:00.131192   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0110 22:12:00.200086   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0110 22:12:00.264319   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0110 22:12:00.332434   23180 ssh_runner.go:362] scp C:\Users\nhath\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0110 22:12:00.401435   23180 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0110 22:12:00.466544   23180 ssh_runner.go:195] Run: openssl version
I0110 22:12:00.484438   23180 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0110 22:12:00.526022   23180 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0110 22:12:00.539077   23180 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan  5 07:51 /usr/share/ca-certificates/minikubeCA.pem
I0110 22:12:00.554646   23180 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0110 22:12:00.572735   23180 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0110 22:12:00.614355   23180 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0110 22:12:00.625086   23180 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0110 22:12:00.625086   23180 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.45 Memory:5900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\nhath:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0110 22:12:00.630010   23180 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0110 22:12:00.673342   23180 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0110 22:12:00.696237   23180 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0110 22:12:00.717883   23180 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0110 22:12:00.719089   23180 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0110 22:12:00.740219   23180 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0110 22:12:00.740219   23180 kubeadm.go:157] found existing configuration files:

I0110 22:12:00.740735   23180 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0110 22:12:00.764241   23180 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0110 22:12:00.764829   23180 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0110 22:12:00.787315   23180 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0110 22:12:00.812826   23180 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0110 22:12:00.813980   23180 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0110 22:12:00.836491   23180 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0110 22:12:00.859210   23180 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0110 22:12:00.860342   23180 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0110 22:12:00.883965   23180 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0110 22:12:00.906771   23180 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0110 22:12:00.907871   23180 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0110 22:12:00.929194   23180 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.31.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0110 22:12:01.013378   23180 kubeadm.go:310] W0110 15:12:00.955974    1991 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "ClusterConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0110 22:12:01.013938   23180 kubeadm.go:310] W0110 15:12:00.957183    1991 common.go:101] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta3" (kind: "InitConfiguration"). Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
I0110 22:12:01.058830   23180 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0110 22:12:01.222602   23180 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0110 22:12:17.177667   23180 kubeadm.go:310] [init] Using Kubernetes version: v1.31.0
I0110 22:12:17.178260   23180 kubeadm.go:310] [preflight] Running pre-flight checks
I0110 22:12:17.178260   23180 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0110 22:12:17.178825   23180 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0110 22:12:17.178825   23180 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0110 22:12:17.178825   23180 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0110 22:12:17.180578   23180 out.go:235]     ▪ Generating certificates and keys ...
I0110 22:12:17.182412   23180 kubeadm.go:310] [certs] Using existing ca certificate authority
I0110 22:12:17.182412   23180 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0110 22:12:17.183018   23180 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0110 22:12:17.183018   23180 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0110 22:12:17.183018   23180 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0110 22:12:17.183018   23180 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0110 22:12:17.183587   23180 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0110 22:12:17.183587   23180 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0110 22:12:17.184158   23180 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0110 22:12:17.184737   23180 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0110 22:12:17.184737   23180 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0110 22:12:17.184737   23180 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0110 22:12:17.184737   23180 kubeadm.go:310] [certs] Generating "sa" key and public key
I0110 22:12:17.185314   23180 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0110 22:12:17.185314   23180 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0110 22:12:17.185871   23180 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0110 22:12:17.185871   23180 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0110 22:12:17.185871   23180 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0110 22:12:17.186456   23180 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0110 22:12:17.186456   23180 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0110 22:12:17.186456   23180 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0110 22:12:17.188745   23180 out.go:235]     ▪ Booting up control plane ...
I0110 22:12:17.189340   23180 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0110 22:12:17.189924   23180 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0110 22:12:17.190551   23180 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0110 22:12:17.191796   23180 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0110 22:12:17.191796   23180 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0110 22:12:17.192372   23180 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0110 22:12:17.192372   23180 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0110 22:12:17.192946   23180 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0110 22:12:17.192946   23180 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.002462382s
I0110 22:12:17.193511   23180 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0110 22:12:17.193511   23180 kubeadm.go:310] [api-check] The API server is healthy after 9.004523023s
I0110 22:12:17.194089   23180 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0110 22:12:17.194089   23180 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0110 22:12:17.194657   23180 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0110 22:12:17.197144   23180 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0110 22:12:17.198171   23180 kubeadm.go:310] [bootstrap-token] Using token: as2k8q.g2sw0kuyzu127d2d
I0110 22:12:17.199762   23180 out.go:235]     ▪ Configuring RBAC rules ...
I0110 22:12:17.200323   23180 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0110 22:12:17.200879   23180 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0110 22:12:17.200879   23180 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0110 22:12:17.200879   23180 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0110 22:12:17.201895   23180 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0110 22:12:17.201895   23180 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0110 22:12:17.202406   23180 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0110 22:12:17.202406   23180 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0110 22:12:17.202406   23180 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0110 22:12:17.202406   23180 kubeadm.go:310] 
I0110 22:12:17.202957   23180 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0110 22:12:17.202957   23180 kubeadm.go:310] 
I0110 22:12:17.202957   23180 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0110 22:12:17.202957   23180 kubeadm.go:310] 
I0110 22:12:17.202957   23180 kubeadm.go:310]   mkdir -p $HOME/.kube
I0110 22:12:17.203499   23180 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0110 22:12:17.203499   23180 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0110 22:12:17.203499   23180 kubeadm.go:310] 
I0110 22:12:17.203499   23180 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0110 22:12:17.203499   23180 kubeadm.go:310] 
I0110 22:12:17.203499   23180 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0110 22:12:17.203499   23180 kubeadm.go:310] 
I0110 22:12:17.203499   23180 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0110 22:12:17.204049   23180 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0110 22:12:17.204049   23180 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0110 22:12:17.204049   23180 kubeadm.go:310] 
I0110 22:12:17.204049   23180 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0110 22:12:17.204599   23180 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0110 22:12:17.204599   23180 kubeadm.go:310] 
I0110 22:12:17.204599   23180 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token as2k8q.g2sw0kuyzu127d2d \
I0110 22:12:17.205138   23180 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:639c54af9b5c91be96850d5bc329d34b11a37ae9829c1b9ea67cff5da3953bcb \
I0110 22:12:17.205138   23180 kubeadm.go:310] 	--control-plane 
I0110 22:12:17.205138   23180 kubeadm.go:310] 
I0110 22:12:17.205138   23180 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0110 22:12:17.205138   23180 kubeadm.go:310] 
I0110 22:12:17.205694   23180 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token as2k8q.g2sw0kuyzu127d2d \
I0110 22:12:17.205694   23180 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:639c54af9b5c91be96850d5bc329d34b11a37ae9829c1b9ea67cff5da3953bcb 
I0110 22:12:17.205694   23180 cni.go:84] Creating CNI manager for ""
I0110 22:12:17.205694   23180 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0110 22:12:17.207401   23180 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0110 22:12:17.211766   23180 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0110 22:12:17.281876   23180 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0110 22:12:17.451198   23180 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0110 22:12:17.452983   23180 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0110 22:12:17.452983   23180 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_01_10T22_12_17_0700 minikube.k8s.io/version=v1.34.0 minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0110 22:12:17.476524   23180 ops.go:34] apiserver oom_adj: -16
I0110 22:12:17.749889   23180 kubeadm.go:1113] duration metric: took 298.6907ms to wait for elevateKubeSystemPrivileges
I0110 22:12:17.824659   23180 kubeadm.go:394] duration metric: took 17.1995736s to StartCluster
I0110 22:12:17.825221   23180 settings.go:142] acquiring lock: {Name:mke8ddc74caaa22c7c51583f5b70aa736680ff05 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:12:17.825221   23180 settings.go:150] Updating kubeconfig:  C:\Users\nhath\.kube\config
I0110 22:12:17.826961   23180 lock.go:35] WriteFile acquiring C:\Users\nhath\.kube\config: {Name:mk8171472f3a0925aa472039be99e0669032fda4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 22:12:17.828102   23180 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0110 22:12:17.828102   23180 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0110 22:12:17.828102   23180 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0110 22:12:17.828668   23180 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0110 22:12:17.828668   23180 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0110 22:12:17.828668   23180 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0110 22:12:17.828668   23180 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0110 22:12:17.829568   23180 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I0110 22:12:17.830204   23180 host.go:66] Checking if "minikube" exists ...
I0110 22:12:17.832022   23180 out.go:177] 🔎  Verifying Kubernetes components...
I0110 22:12:17.834994   23180 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 22:12:17.851353   23180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 22:12:17.851353   23180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 22:12:17.955428   23180 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0110 22:12:17.955428   23180 host.go:66] Checking if "minikube" exists ...
I0110 22:12:17.959853   23180 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0110 22:12:17.961454   23180 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0110 22:12:17.961454   23180 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0110 22:12:17.967451   23180 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 22:12:17.969094   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:12:18.057198   23180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53375 SSHKeyPath:C:\Users\nhath\.minikube\machines\minikube\id_rsa Username:docker}
I0110 22:12:18.064057   23180 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I0110 22:12:18.064057   23180 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0110 22:12:18.069577   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 22:12:18.149750   23180 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53375 SSHKeyPath:C:\Users\nhath\.minikube\machines\minikube\id_rsa Username:docker}
I0110 22:12:18.325354   23180 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.31.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0110 22:12:18.398985   23180 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0110 22:12:18.500318   23180 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0110 22:12:18.607183   23180 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0110 22:12:19.190186   23180 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0110 22:12:19.195388   23180 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0110 22:12:19.281848   23180 api_server.go:52] waiting for apiserver process to appear ...
I0110 22:12:19.282433   23180 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0110 22:12:19.573387   23180 api_server.go:72] duration metric: took 1.7452845s to wait for apiserver process to appear ...
I0110 22:12:19.573387   23180 api_server.go:88] waiting for apiserver healthz status ...
I0110 22:12:19.573387   23180 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.0730683s)
I0110 22:12:19.573387   23180 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53374/healthz ...
I0110 22:12:19.589864   23180 api_server.go:279] https://127.0.0.1:53374/healthz returned 200:
ok
I0110 22:12:19.591584   23180 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0110 22:12:19.593894   23180 api_server.go:141] control plane version: v1.31.0
I0110 22:12:19.593894   23180 api_server.go:131] duration metric: took 20.5074ms to wait for apiserver health ...
I0110 22:12:19.593894   23180 system_pods.go:43] waiting for kube-system pods to appear ...
I0110 22:12:19.594454   23180 addons.go:510] duration metric: took 1.7663515s for enable addons: enabled=[storage-provisioner default-storageclass]
I0110 22:12:19.603431   23180 system_pods.go:59] 5 kube-system pods found
I0110 22:12:19.603431   23180 system_pods.go:61] "etcd-minikube" [2a866d54-a74c-4e19-be3a-bf297afbaa71] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0110 22:12:19.603431   23180 system_pods.go:61] "kube-apiserver-minikube" [11316413-8b36-4bb8-b587-91041fef4a51] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0110 22:12:19.603431   23180 system_pods.go:61] "kube-controller-manager-minikube" [83b9739b-05fc-4a9a-8163-1f04e7db7215] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0110 22:12:19.603431   23180 system_pods.go:61] "kube-scheduler-minikube" [907a80ca-9898-4762-9e7e-e45193265650] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0110 22:12:19.603431   23180 system_pods.go:61] "storage-provisioner" [978f1e22-b1c8-4e87-b035-02401f6037be] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0110 22:12:19.603431   23180 system_pods.go:74] duration metric: took 9.537ms to wait for pod list to return data ...
I0110 22:12:19.603431   23180 kubeadm.go:582] duration metric: took 1.7753289s to wait for: map[apiserver:true system_pods:true]
I0110 22:12:19.603431   23180 node_conditions.go:102] verifying NodePressure condition ...
I0110 22:12:19.610303   23180 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0110 22:12:19.610303   23180 node_conditions.go:123] node cpu capacity is 12
I0110 22:12:19.610303   23180 node_conditions.go:105] duration metric: took 6.8716ms to run NodePressure ...
I0110 22:12:19.610303   23180 start.go:241] waiting for startup goroutines ...
I0110 22:12:19.706694   23180 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0110 22:12:19.706694   23180 start.go:246] waiting for cluster config update ...
I0110 22:12:19.706694   23180 start.go:255] writing updated cluster config ...
I0110 22:12:19.721386   23180 ssh_runner.go:195] Run: rm -f paused
I0110 22:12:19.864169   23180 start.go:600] kubectl: 1.30.5, cluster: 1.31.0 (minor skew: 1)
I0110 22:12:19.866610   23180 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 10 15:13:52 minikube cri-dockerd[1640]: time="2025-01-10T15:13:52Z" level=info msg="Pulling image nirajsonawane/student-app-api:0.0.1-SNAPSHOT: c2274a1a0e27: Downloading [====================================>              ]  51.23MB/70.73MB"
Jan 10 15:14:02 minikube cri-dockerd[1640]: time="2025-01-10T15:14:02Z" level=info msg="Pulling image nirajsonawane/student-app-api:0.0.1-SNAPSHOT: 2be5f6dc2f5f: Downloading [==============================================>    ]  22.38MB/23.82MB"
Jan 10 15:14:07 minikube cri-dockerd[1640]: time="2025-01-10T15:14:07Z" level=info msg="Stop pulling image nirajsonawane/student-app-api:0.0.1-SNAPSHOT: Status: Downloaded newer image for nirajsonawane/student-app-api:0.0.1-SNAPSHOT"
Jan 10 15:14:21 minikube cri-dockerd[1640]: time="2025-01-10T15:14:21Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 99760bc62448: Downloading [======>                                            ]  5.956MB/45.38MB"
Jan 10 15:14:31 minikube cri-dockerd[1640]: time="2025-01-10T15:14:31Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 99760bc62448: Downloading [=============>                                     ]  11.94MB/45.38MB"
Jan 10 15:14:40 minikube cri-dockerd[1640]: time="2025-01-10T15:14:40Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: c1f89293f045: Downloading [=================================================> ]  49.27MB/50.08MB"
Jan 10 15:14:50 minikube cri-dockerd[1640]: time="2025-01-10T15:14:50Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: be1dbde81db8: Downloading [=======>                                           ]  3.417MB/23.75MB"
Jan 10 15:15:00 minikube cri-dockerd[1640]: time="2025-01-10T15:15:00Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: e3a94252463d: Download complete "
Jan 10 15:15:08 minikube cri-dockerd[1640]: time="2025-01-10T15:15:08Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: c1f89293f045: Extracting [=================================>                 ]  33.55MB/50.08MB"
Jan 10 15:15:18 minikube cri-dockerd[1640]: time="2025-01-10T15:15:18Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: b1af2b8c856c: Downloading [===============================>                   ]  7.877MB/12.58MB"
Jan 10 15:15:18 minikube dockerd[1367]: time="2025-01-10T15:15:18.821988671Z" level=info msg="ignoring event" container=050705ef1a540dd00de778873673d229f3896ae5e8ee41efd43f793cdd2db5d4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 10 15:15:28 minikube cri-dockerd[1640]: time="2025-01-10T15:15:28Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: b1af2b8c856c: Download complete "
Jan 10 15:15:37 minikube cri-dockerd[1640]: time="2025-01-10T15:15:37Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [========>                                          ]  16.55MB/102.4MB"
Jan 10 15:15:45 minikube cri-dockerd[1640]: time="2025-01-10T15:15:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a7dd001836839417fef76f7cd74987eefe659277bca9fc31b6af84b02493f261/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 10 15:15:45 minikube cri-dockerd[1640]: time="2025-01-10T15:15:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9f29c8d00782e131336633fed0974d08c5ae037dee8a5b48457c07637a339e34/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 10 15:15:47 minikube cri-dockerd[1640]: time="2025-01-10T15:15:47Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [==========>                                        ]  21.88MB/102.4MB"
Jan 10 15:15:57 minikube cri-dockerd[1640]: time="2025-01-10T15:15:57Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=====>                                             ]  22.47MB/214.9MB"
Jan 10 15:16:05 minikube cri-dockerd[1640]: time="2025-01-10T15:16:05Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [==============>                                    ]  29.36MB/102.4MB"
Jan 10 15:16:15 minikube cri-dockerd[1640]: time="2025-01-10T15:16:15Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [===============>                                   ]  32.06MB/102.4MB"
Jan 10 15:16:25 minikube cri-dockerd[1640]: time="2025-01-10T15:16:25Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=================>                                 ]  35.83MB/102.4MB"
Jan 10 15:16:33 minikube cri-dockerd[1640]: time="2025-01-10T15:16:33Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=====>                                             ]  23.54MB/214.9MB"
Jan 10 15:16:43 minikube cri-dockerd[1640]: time="2025-01-10T15:16:43Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [====================>                              ]   41.2MB/102.4MB"
Jan 10 15:16:53 minikube cri-dockerd[1640]: time="2025-01-10T15:16:53Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=====================>                             ]  43.89MB/102.4MB"
Jan 10 15:17:02 minikube cri-dockerd[1640]: time="2025-01-10T15:17:02Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [======================>                            ]   45.5MB/102.4MB"
Jan 10 15:17:12 minikube cri-dockerd[1640]: time="2025-01-10T15:17:12Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=======================>                           ]  47.66MB/102.4MB"
Jan 10 15:17:22 minikube cri-dockerd[1640]: time="2025-01-10T15:17:22Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [========================>                          ]   49.8MB/102.4MB"
Jan 10 15:17:30 minikube cri-dockerd[1640]: time="2025-01-10T15:17:30Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=========================>                         ]  51.95MB/102.4MB"
Jan 10 15:17:40 minikube cri-dockerd[1640]: time="2025-01-10T15:17:40Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=========================>                         ]  53.03MB/102.4MB"
Jan 10 15:17:50 minikube cri-dockerd[1640]: time="2025-01-10T15:17:50Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [==========================>                        ]  54.64MB/102.4MB"
Jan 10 15:17:58 minikube cri-dockerd[1640]: time="2025-01-10T15:17:58Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [===========================>                       ]  56.26MB/102.4MB"
Jan 10 15:18:08 minikube cri-dockerd[1640]: time="2025-01-10T15:18:08Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [============================>                      ]  57.88MB/102.4MB"
Jan 10 15:18:18 minikube cri-dockerd[1640]: time="2025-01-10T15:18:18Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=============================>                     ]  59.47MB/102.4MB"
Jan 10 15:18:26 minikube cri-dockerd[1640]: time="2025-01-10T15:18:26Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=============================>                     ]  61.08MB/102.4MB"
Jan 10 15:18:36 minikube cri-dockerd[1640]: time="2025-01-10T15:18:36Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [======>                                            ]  28.39MB/214.9MB"
Jan 10 15:18:46 minikube cri-dockerd[1640]: time="2025-01-10T15:18:46Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [===============================>                   ]  63.76MB/102.4MB"
Jan 10 15:18:55 minikube cri-dockerd[1640]: time="2025-01-10T15:18:55Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [===============================>                   ]  64.83MB/102.4MB"
Jan 10 15:19:05 minikube cri-dockerd[1640]: time="2025-01-10T15:19:05Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [================================>                  ]  66.45MB/102.4MB"
Jan 10 15:19:15 minikube cri-dockerd[1640]: time="2025-01-10T15:19:15Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [================================>                  ]  67.51MB/102.4MB"
Jan 10 15:19:23 minikube cri-dockerd[1640]: time="2025-01-10T15:19:23Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [=================================>                 ]  69.12MB/102.4MB"
Jan 10 15:19:33 minikube cri-dockerd[1640]: time="2025-01-10T15:19:33Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [==================================>                ]  71.27MB/102.4MB"
Jan 10 15:19:43 minikube cri-dockerd[1640]: time="2025-01-10T15:19:43Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [===================================>               ]  72.88MB/102.4MB"
Jan 10 15:19:51 minikube cri-dockerd[1640]: time="2025-01-10T15:19:51Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [====================================>              ]  75.57MB/102.4MB"
Jan 10 15:20:01 minikube cri-dockerd[1640]: time="2025-01-10T15:20:01Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [======================================>            ]  78.25MB/102.4MB"
Jan 10 15:20:11 minikube cri-dockerd[1640]: time="2025-01-10T15:20:11Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [======================================>            ]  79.85MB/102.4MB"
Jan 10 15:20:20 minikube cri-dockerd[1640]: time="2025-01-10T15:20:20Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [========================================>          ]  83.06MB/102.4MB"
Jan 10 15:20:30 minikube cri-dockerd[1640]: time="2025-01-10T15:20:30Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [==========================================>        ]  87.86MB/102.4MB"
Jan 10 15:20:40 minikube cri-dockerd[1640]: time="2025-01-10T15:20:40Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Downloading [============================================>      ]  92.12MB/102.4MB"
Jan 10 15:20:48 minikube cri-dockerd[1640]: time="2025-01-10T15:20:48Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 9e24c300c46e: Download complete "
Jan 10 15:20:58 minikube cri-dockerd[1640]: time="2025-01-10T15:20:58Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [========>                                          ]  37.55MB/214.9MB"
Jan 10 15:21:08 minikube cri-dockerd[1640]: time="2025-01-10T15:21:08Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [========>                                          ]  37.55MB/214.9MB"
Jan 10 15:21:16 minikube cri-dockerd[1640]: time="2025-01-10T15:21:16Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [========>                                          ]  38.09MB/214.9MB"
Jan 10 15:21:26 minikube cri-dockerd[1640]: time="2025-01-10T15:21:26Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [========>                                          ]  38.09MB/214.9MB"
Jan 10 15:21:36 minikube cri-dockerd[1640]: time="2025-01-10T15:21:36Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [========>                                          ]  38.63MB/214.9MB"
Jan 10 15:21:44 minikube cri-dockerd[1640]: time="2025-01-10T15:21:44Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=========>                                         ]  39.17MB/214.9MB"
Jan 10 15:21:54 minikube cri-dockerd[1640]: time="2025-01-10T15:21:54Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=========>                                         ]  39.17MB/214.9MB"
Jan 10 15:22:04 minikube cri-dockerd[1640]: time="2025-01-10T15:22:04Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=========>                                         ]  39.71MB/214.9MB"
Jan 10 15:22:13 minikube cri-dockerd[1640]: time="2025-01-10T15:22:13Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=========>                                         ]  39.71MB/214.9MB"
Jan 10 15:22:23 minikube cri-dockerd[1640]: time="2025-01-10T15:22:23Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=========>                                         ]  40.24MB/214.9MB"
Jan 10 15:22:33 minikube cri-dockerd[1640]: time="2025-01-10T15:22:33Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=========>                                         ]  40.78MB/214.9MB"
Jan 10 15:22:41 minikube cri-dockerd[1640]: time="2025-01-10T15:22:41Z" level=info msg="Pulling image nirajsonawane/student-app-client:latest: 115b6fc5ace1: Downloading [=========>                                         ]  40.78MB/214.9MB"


==> container status <==
CONTAINER           IMAGE                                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
050705ef1a540       nirajsonawane/student-app-api@sha256:4b6f613418d0cf848377e04978b0afa6191dd9c19ae189cba6f86aac29895c9d   8 minutes ago       Exited              student-app-api           0                   d1ed9ce093984       student-app-api-744bb4c9b8-zqsn5
da807a1b8079c       6e38f40d628db                                                                                           10 minutes ago      Running             storage-provisioner       1                   6403ab3e808eb       storage-provisioner
936c0092e636e       cbb01a7bd410d                                                                                           10 minutes ago      Running             coredns                   0                   ba808a71b056a       coredns-6f6b679f8f-k7jlm
08673480523e6       ad83b2ca7b09e                                                                                           10 minutes ago      Running             kube-proxy                0                   5a46c5171600d       kube-proxy-r2kff
cf2938647d1ca       6e38f40d628db                                                                                           10 minutes ago      Exited              storage-provisioner       0                   6403ab3e808eb       storage-provisioner
017a06cdd832c       604f5db92eaa8                                                                                           10 minutes ago      Running             kube-apiserver            0                   8ed6b4fe8ae34       kube-apiserver-minikube
950bf9c262e52       045733566833c                                                                                           10 minutes ago      Running             kube-controller-manager   0                   be59b9ab95f6b       kube-controller-manager-minikube
2e40ed9a31b86       1766f54c897f0                                                                                           10 minutes ago      Running             kube-scheduler            0                   7de06141af2b3       kube-scheduler-minikube
ea01afb74baa3       2e96e5913fc06                                                                                           10 minutes ago      Running             etcd                      0                   960d753d88029       etcd-minikube


==> coredns [936c0092e636] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:43024 - 55206 "HINFO IN 1612489608682028316.8230140122429394605. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.131907632s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1600505674]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jan-2025 15:12:22.777) (total time: 22240ms):
Trace[1600505674]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22239ms (15:12:43.434)
Trace[1600505674]: [22.24004297s] [22.24004297s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1048866076]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jan-2025 15:12:22.777) (total time: 22240ms):
Trace[1048866076]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22239ms (15:12:43.434)
Trace[1048866076]: [22.2402565s] [22.2402565s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1965361189]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (10-Jan-2025 15:12:22.777) (total time: 22240ms):
Trace[1965361189]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22239ms (15:12:43.434)
Trace[1965361189]: [22.240451952s] [22.240451952s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] 10.244.0.3:44010 - 35137 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.001595429s
[INFO] 10.244.0.3:44010 - 34385 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.002112154s
[INFO] 10.244.0.3:44516 - 53718 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 96 0.000366035s
[INFO] 10.244.0.3:44516 - 54319 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NOERROR qr,aa,rd 142 0.000766148s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_01_10T22_12_17_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 10 Jan 2025 15:12:13 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 10 Jan 2025 15:22:43 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 10 Jan 2025 15:19:03 +0000   Fri, 10 Jan 2025 15:12:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 10 Jan 2025 15:19:03 +0000   Fri, 10 Jan 2025 15:12:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 10 Jan 2025 15:19:03 +0000   Fri, 10 Jan 2025 15:12:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 10 Jan 2025 15:19:03 +0000   Fri, 10 Jan 2025 15:12:14 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             11908272Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             11908272Ki
  pods:               110
System Info:
  Machine ID:                 8356bd0920914888820790ebae30a2e8
  System UUID:                8356bd0920914888820790ebae30a2e8
  Boot ID:                    263326ae-9bcf-4025-bd05-a2dd9a860049
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     mongo-5df4f657ff-vtgpv                      250m (2%)     500m (4%)   256Mi (2%)       512Mi (4%)     9m19s
  default                     student-app-api-744bb4c9b8-zqsn5            250m (2%)     500m (4%)   256Mi (2%)       512Mi (4%)     9m18s
  default                     student-app-client-676874d64b-svh22         250m (2%)     500m (4%)   256Mi (2%)       512Mi (4%)     9m18s
  ingress-nginx               ingress-nginx-admission-create-xl5gm        0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m2s
  ingress-nginx               ingress-nginx-admission-patch-9pjlv         0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m2s
  ingress-nginx               ingress-nginx-controller-bc57996ff-gdmzx    100m (0%)     0 (0%)      90Mi (0%)        0 (0%)         7m2s
  kube-system                 coredns-6f6b679f8f-k7jlm                    100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     10m
  kube-system                 etcd-minikube                               100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         10m
  kube-system                 kube-apiserver-minikube                     250m (2%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-controller-manager-minikube            200m (1%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-proxy-r2kff                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-scheduler-minikube                     100m (0%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 storage-provisioner                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1600m (13%)  1500m (12%)
  memory             1028Mi (8%)  1706Mi (14%)
  ephemeral-storage  0 (0%)       0 (0%)
  hugepages-1Gi      0 (0%)       0 (0%)
  hugepages-2Mi      0 (0%)       0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           10m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  10m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           10m                kubelet          Starting kubelet.
  Warning  CgroupV1                           10m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            10m (x7 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              10m (x7 over 10m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               10m (x7 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            10m                kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  10m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           10m                kubelet          Starting kubelet.
  Warning  CgroupV1                           10m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            10m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            10m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              10m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               10m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     10m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.001765] FS-Cache: O-key=[8] 'bed3090000001300'
[  +0.000636] FS-Cache: N-cookie c=0000000a [p=00000006 fl=2 nc=0 na=1]
[  +0.000781] FS-Cache: N-cookie d=00000000fc6b9929{9p.inode} n=0000000088f9a309
[  +0.001089] FS-Cache: N-key=[8] 'bed3090000001300'
[  +0.100070] FS-Cache: Duplicate cookie detected
[  +0.001230] FS-Cache: O-cookie c=0000000c [p=00000006 fl=226 nc=0 na=1]
[  +0.001361] FS-Cache: O-cookie d=00000000fc6b9929{9p.inode} n=000000002e078ed5
[  +0.001536] FS-Cache: O-key=[8] 'c9d7090000001500'
[  +0.001077] FS-Cache: N-cookie c=0000000d [p=00000006 fl=2 nc=0 na=1]
[  +0.001286] FS-Cache: N-cookie d=00000000fc6b9929{9p.inode} n=000000007c3fb440
[  +0.001601] FS-Cache: N-key=[8] 'c9d7090000001500'
[  +0.157682] FS-Cache: Duplicate cookie detected
[  +0.000828] FS-Cache: O-cookie c=0000000f [p=00000006 fl=226 nc=0 na=1]
[  +0.000944] FS-Cache: O-cookie d=00000000fc6b9929{9p.inode} n=00000000952080e1
[  +0.000749] FS-Cache: O-key=[8] '24d8090000002100'
[  +0.000510] FS-Cache: N-cookie c=00000010 [p=00000006 fl=2 nc=0 na=1]
[  +0.000658] FS-Cache: N-cookie d=00000000fc6b9929{9p.inode} n=000000005ee27a4f
[  +0.000837] FS-Cache: N-key=[8] '24d8090000002100'
[  +0.073550] FS-Cache: Duplicate cookie detected
[  +0.000738] FS-Cache: O-cookie c=00000012 [p=00000006 fl=226 nc=0 na=1]
[  +0.000712] FS-Cache: O-cookie d=00000000fc6b9929{9p.inode} n=00000000bfb5ae92
[  +0.001683] FS-Cache: O-key=[8] 'f79a080000001000'
[  +0.000701] FS-Cache: N-cookie c=00000013 [p=00000006 fl=2 nc=0 na=1]
[  +0.001241] FS-Cache: N-cookie d=00000000fc6b9929{9p.inode} n=00000000de6bf5d5
[  +0.001107] FS-Cache: N-key=[8] 'f79a080000001000'
[  +0.709921] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.021100] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Saigon not found. Is the tzdata package installed?
[  +0.878277] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.532017] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001920] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005637] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003893] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.005573] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002411] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005247] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.005235] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.914699] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.088616] netlink: 'init': attribute type 4 has an invalid length.
[  +0.026559] WSL (2) WARNING: /usr/share/zoneinfo/Asia/Saigon not found. Is the tzdata package installed?
[  +0.384205] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.014422] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.008828] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001225] new mount options do not match the existing superblock, will be ignored
[  +0.006241] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.013370] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.006010] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.017195] Failed to connect to bus: No such file or directory
[  +0.007142] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.008266] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.315480] Failed to connect to bus: No such file or directory
[  +0.282543] Failed to connect to bus: No such file or directory
[  +0.273896] Failed to connect to bus: No such file or directory
[  +0.270036] Failed to connect to bus: No such file or directory
[  +0.279401] Failed to connect to bus: No such file or directory
[  +0.914473] systemd-journald[66]: File /var/log/journal/23d2811785744f55b687196c7360c80f/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Jan10 14:48] tmpfs: Unknown parameter 'noswap'
[ +10.842108] tmpfs: Unknown parameter 'noswap'
[Jan10 15:09] tmpfs: Unknown parameter 'noswap'
[Jan10 15:10] tmpfs: Unknown parameter 'noswap'


==> etcd [ea01afb74baa] <==
{"level":"info","ts":"2025-01-10T15:12:09.394131Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-01-10T15:12:09.394164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-01-10T15:12:09.394301Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-01-10T15:12:09.464505Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-01-10T15:12:09.473612Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-01-10T15:12:09.480021Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-01-10T15:12:09.493351Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-01-10T15:12:09.493819Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-01-10T15:12:09.493985Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-10T15:12:09.494395Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-10T15:12:09.494478Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-10T15:12:09.557706Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-10T15:12:09.558911Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-01-10T15:12:09.559233Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-01-10T15:12:09.563022Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-01-10T15:12:09.563573Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-10T15:12:09.563663Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-01-10T15:12:09.563702Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-10T15:12:09.563574Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-01-10T15:12:09.695176Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-01-10T15:12:09.695320Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-01-10T15:12:09.695396Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-01-10T15:12:09.695426Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-01-10T15:12:09.695436Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-01-10T15:12:09.695450Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-01-10T15:12:09.695462Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-01-10T15:12:09.758259Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-10T15:12:09.760546Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-01-10T15:12:09.760607Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-10T15:12:09.760716Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-10T15:12:09.762847Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-10T15:12:09.763019Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-10T15:12:09.763222Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-10T15:12:09.762902Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-01-10T15:12:09.763373Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-10T15:12:09.763397Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-01-10T15:12:09.763432Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-10T15:12:09.766311Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-01-10T15:12:09.767815Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"warn","ts":"2025-01-10T15:12:14.078103Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.667433ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/certificatesigningrequests\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-01-10T15:12:14.078340Z","caller":"traceutil/trace.go:171","msg":"trace[250638663] range","detail":"{range_begin:/registry/certificatesigningrequests; range_end:; response_count:0; response_revision:18; }","duration":"100.928543ms","start":"2025-01-10T15:12:13.977385Z","end":"2025-01-10T15:12:14.078314Z","steps":["trace[250638663] 'agreement among raft nodes before linearized reading'  (duration: 85.086924ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-10T15:12:58.375924Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.7556ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128034502134117960 > lease_revoke:<id:70cc9450c5d2b5ed>","response":"size:29"}
{"level":"info","ts":"2025-01-10T15:13:05.550836Z","caller":"traceutil/trace.go:171","msg":"trace[631335747] transaction","detail":"{read_only:false; response_revision:450; number_of_response:1; }","duration":"345.941813ms","start":"2025-01-10T15:13:05.204873Z","end":"2025-01-10T15:13:05.550815Z","steps":["trace[631335747] 'process raft request'  (duration: 345.806472ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-10T15:13:05.552460Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-01-10T15:13:05.204847Z","time spent":"346.155913ms","remote":"127.0.0.1:47586","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:442 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-01-10T15:14:44.926057Z","caller":"traceutil/trace.go:171","msg":"trace[579967295] transaction","detail":"{read_only:false; response_revision:605; number_of_response:1; }","duration":"228.694767ms","start":"2025-01-10T15:14:44.697340Z","end":"2025-01-10T15:14:44.926035Z","steps":["trace[579967295] 'process raft request'  (duration: 228.498502ms)"],"step_count":1}
{"level":"info","ts":"2025-01-10T15:14:59.392838Z","caller":"traceutil/trace.go:171","msg":"trace[343807912] transaction","detail":"{read_only:false; response_revision:617; number_of_response:1; }","duration":"221.735423ms","start":"2025-01-10T15:14:59.171076Z","end":"2025-01-10T15:14:59.392811Z","steps":["trace[343807912] 'process raft request'  (duration: 221.300125ms)"],"step_count":1}
{"level":"info","ts":"2025-01-10T15:15:36.699577Z","caller":"traceutil/trace.go:171","msg":"trace[1660793043] transaction","detail":"{read_only:false; response_revision:653; number_of_response:1; }","duration":"191.23702ms","start":"2025-01-10T15:15:36.508309Z","end":"2025-01-10T15:15:36.699546Z","steps":["trace[1660793043] 'process raft request'  (duration: 103.764206ms)","trace[1660793043] 'compare'  (duration: 87.119267ms)"],"step_count":2}
{"level":"info","ts":"2025-01-10T15:16:01.704814Z","caller":"traceutil/trace.go:171","msg":"trace[2146408916] transaction","detail":"{read_only:false; response_revision:737; number_of_response:1; }","duration":"114.956086ms","start":"2025-01-10T15:16:01.589832Z","end":"2025-01-10T15:16:01.704788Z","steps":["trace[2146408916] 'process raft request'  (duration: 69.93863ms)","trace[2146408916] 'compare'  (duration: 44.870241ms)"],"step_count":2}
{"level":"info","ts":"2025-01-10T15:16:48.307220Z","caller":"traceutil/trace.go:171","msg":"trace[966079407] transaction","detail":"{read_only:false; response_revision:776; number_of_response:1; }","duration":"157.049878ms","start":"2025-01-10T15:16:48.150143Z","end":"2025-01-10T15:16:48.307193Z","steps":["trace[966079407] 'process raft request'  (duration: 156.907984ms)"],"step_count":1}
{"level":"info","ts":"2025-01-10T15:18:57.224343Z","caller":"traceutil/trace.go:171","msg":"trace[742652393] transaction","detail":"{read_only:false; response_revision:883; number_of_response:1; }","duration":"190.172029ms","start":"2025-01-10T15:18:57.034150Z","end":"2025-01-10T15:18:57.224322Z","steps":["trace[742652393] 'process raft request'  (duration: 190.049879ms)"],"step_count":1}
{"level":"info","ts":"2025-01-10T15:19:02.443260Z","caller":"traceutil/trace.go:171","msg":"trace[1975608158] transaction","detail":"{read_only:false; response_revision:888; number_of_response:1; }","duration":"117.445991ms","start":"2025-01-10T15:19:02.325790Z","end":"2025-01-10T15:19:02.443236Z","steps":["trace[1975608158] 'process raft request'  (duration: 117.17413ms)"],"step_count":1}
{"level":"info","ts":"2025-01-10T15:19:03.630242Z","caller":"traceutil/trace.go:171","msg":"trace[681672539] transaction","detail":"{read_only:false; response_revision:890; number_of_response:1; }","duration":"140.392794ms","start":"2025-01-10T15:19:03.489810Z","end":"2025-01-10T15:19:03.630203Z","steps":["trace[681672539] 'process raft request'  (duration: 139.992905ms)"],"step_count":1}
{"level":"info","ts":"2025-01-10T15:19:17.737235Z","caller":"traceutil/trace.go:171","msg":"trace[551453267] transaction","detail":"{read_only:false; response_revision:900; number_of_response:1; }","duration":"234.985493ms","start":"2025-01-10T15:19:17.502227Z","end":"2025-01-10T15:19:17.737212Z","steps":["trace[551453267] 'process raft request'  (duration: 234.85259ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-10T15:19:20.499579Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"245.96845ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-10T15:19:20.499835Z","caller":"traceutil/trace.go:171","msg":"trace[1484967384] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:904; }","duration":"246.258697ms","start":"2025-01-10T15:19:20.253544Z","end":"2025-01-10T15:19:20.499802Z","steps":["trace[1484967384] 'range keys from in-memory index tree'  (duration: 245.873428ms)"],"step_count":1}
{"level":"warn","ts":"2025-01-10T15:19:46.647301Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"151.272042ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-01-10T15:19:46.647841Z","caller":"traceutil/trace.go:171","msg":"trace[2041104330] range","detail":"{range_begin:/registry/pods; range_end:; response_count:0; response_revision:925; }","duration":"151.817764ms","start":"2025-01-10T15:19:46.495980Z","end":"2025-01-10T15:19:46.647797Z","steps":["trace[2041104330] 'range keys from in-memory index tree'  (duration: 151.204202ms)"],"step_count":1}
{"level":"info","ts":"2025-01-10T15:21:37.328650Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":780}
{"level":"info","ts":"2025-01-10T15:21:37.359272Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":780,"took":"29.77636ms","hash":3502874105,"current-db-size-bytes":2441216,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":2441216,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-01-10T15:21:37.359378Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3502874105,"revision":780,"compact-revision":-1}


==> kernel <==
 15:22:46 up 43 min,  0 users,  load average: 0.23, 0.53, 0.71
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [017a06cdd832] <==
I0110 15:12:13.796758       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0110 15:12:13.796883       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0110 15:12:13.797004       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0110 15:12:13.797086       1 controller.go:142] Starting OpenAPI controller
I0110 15:12:13.797131       1 controller.go:90] Starting OpenAPI V3 controller
I0110 15:12:13.797154       1 naming_controller.go:294] Starting NamingConditionController
I0110 15:12:13.797189       1 establishing_controller.go:81] Starting EstablishingController
I0110 15:12:13.797212       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0110 15:12:13.797234       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0110 15:12:13.797254       1 crd_finalizer.go:269] Starting CRDFinalizer
I0110 15:12:13.797507       1 aggregator.go:169] waiting for initial CRD sync...
I0110 15:12:13.797533       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0110 15:12:13.797542       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0110 15:12:13.796721       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0110 15:12:13.798485       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I0110 15:12:13.798726       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0110 15:12:13.799092       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0110 15:12:13.799509       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0110 15:12:13.802149       1 controller.go:78] Starting OpenAPI AggregationController
I0110 15:12:13.955867       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0110 15:12:13.955918       1 policy_source.go:224] refreshing policies
I0110 15:12:13.956022       1 cache.go:39] Caches are synced for LocalAvailability controller
I0110 15:12:13.956059       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0110 15:12:13.956176       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I0110 15:12:13.956224       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0110 15:12:13.956243       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0110 15:12:13.956451       1 shared_informer.go:320] Caches are synced for node_authorizer
I0110 15:12:13.956988       1 shared_informer.go:320] Caches are synced for configmaps
I0110 15:12:13.957180       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0110 15:12:13.958510       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0110 15:12:13.958558       1 aggregator.go:171] initial CRD sync complete...
I0110 15:12:13.958593       1 autoregister_controller.go:144] Starting autoregister controller
I0110 15:12:13.958605       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0110 15:12:13.958610       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0110 15:12:13.958615       1 cache.go:39] Caches are synced for autoregister controller
I0110 15:12:13.963373       1 controller.go:615] quota admission added evaluator for: namespaces
E0110 15:12:13.978231       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0110 15:12:14.190364       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0110 15:12:14.815694       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0110 15:12:14.092315       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0110 15:12:14.092392       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0110 15:12:14.287621       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0110 15:12:14.408690       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0110 15:12:14.604359       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0110 15:12:14.627228       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0110 15:12:14.629405       1 controller.go:615] quota admission added evaluator for: endpoints
I0110 15:12:14.641583       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0110 15:12:15.100334       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0110 15:12:15.742414       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0110 15:12:15.792474       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0110 15:12:15.810153       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0110 15:12:20.502631       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0110 15:12:20.648186       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0110 15:13:27.904894       1 alloc.go:330] "allocated clusterIPs" service="default/mongo" clusterIPs={"IPv4":"10.102.148.129"}
I0110 15:13:28.169896       1 alloc.go:330] "allocated clusterIPs" service="default/student-app-api" clusterIPs={"IPv4":"10.108.120.60"}
I0110 15:13:28.303176       1 alloc.go:330] "allocated clusterIPs" service="default/student-app-client-service" clusterIPs={"IPv4":"10.105.227.202"}
I0110 15:13:28.373521       1 controller.go:615] quota admission added evaluator for: ingresses.networking.k8s.io
I0110 15:15:44.667189       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.101.205.115"}
I0110 15:15:44.725349       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.103.43.190"}
I0110 15:15:44.812699       1 controller.go:615] quota admission added evaluator for: jobs.batch


==> kube-controller-manager [950bf9c262e5] <==
I0110 15:12:19.740087       1 shared_informer.go:320] Caches are synced for HPA
I0110 15:12:19.744341       1 shared_informer.go:320] Caches are synced for TTL after finished
I0110 15:12:19.744623       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0110 15:12:19.744735       1 shared_informer.go:320] Caches are synced for service account
I0110 15:12:19.838499       1 shared_informer.go:320] Caches are synced for attach detach
I0110 15:12:19.842612       1 shared_informer.go:320] Caches are synced for disruption
I0110 15:12:19.889454       1 shared_informer.go:320] Caches are synced for resource quota
I0110 15:12:19.893806       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 15:12:19.939833       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0110 15:12:19.940061       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0110 15:12:19.941411       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0110 15:12:19.941498       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0110 15:12:19.941597       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0110 15:12:19.948089       1 shared_informer.go:320] Caches are synced for resource quota
I0110 15:12:20.013028       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0110 15:12:20.391246       1 shared_informer.go:320] Caches are synced for garbage collector
I0110 15:12:20.391365       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0110 15:12:20.414267       1 shared_informer.go:320] Caches are synced for garbage collector
I0110 15:12:20.456981       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 15:12:20.975751       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="464.16326ms"
I0110 15:12:21.084739       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="108.874116ms"
I0110 15:12:21.085190       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="230.471µs"
I0110 15:12:22.870879       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="115.484µs"
I0110 15:12:26.900633       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 15:12:53.234237       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="24.43055ms"
I0110 15:12:53.234567       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="109.747µs"
I0110 15:13:27.825939       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-5df4f657ff" duration="38.272782ms"
I0110 15:13:27.891513       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-5df4f657ff" duration="65.42721ms"
I0110 15:13:27.891767       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-5df4f657ff" duration="94.68µs"
I0110 15:13:28.144760       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="30.658122ms"
I0110 15:13:28.194540       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="49.638973ms"
I0110 15:13:28.194677       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="78.921µs"
I0110 15:13:28.194785       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="46.151µs"
I0110 15:13:28.369840       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-client-676874d64b" duration="88.40631ms"
I0110 15:13:28.384381       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-client-676874d64b" duration="14.40674ms"
I0110 15:13:28.384592       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-client-676874d64b" duration="94.064µs"
I0110 15:13:28.396587       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-client-676874d64b" duration="78.953µs"
I0110 15:13:28.879553       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-5df4f657ff" duration="81.3µs"
I0110 15:13:28.904946       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-5df4f657ff" duration="72.795µs"
I0110 15:14:07.600361       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="16.278844ms"
I0110 15:14:07.600588       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="112.788µs"
I0110 15:14:13.318260       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0110 15:15:20.350469       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="15.591037ms"
I0110 15:15:20.351788       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/student-app-api-744bb4c9b8" duration="423.081µs"
I0110 15:15:44.837472       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0110 15:15:44.905267       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I0110 15:15:44.905353       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="95.91625ms"
I0110 15:15:44.921063       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 15:15:44.943445       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="37.964212ms"
I0110 15:15:44.943508       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 15:15:44.943785       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="198.689µs"
I0110 15:15:44.944044       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="83.007µs"
I0110 15:15:44.958940       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 15:15:44.959012       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 15:15:44.959341       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-bc57996ff" duration="108.149µs"
I0110 15:15:44.971745       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 15:15:44.973359       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 15:15:45.058317       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0110 15:15:45.091959       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0110 15:19:03.632407       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [08673480523e] <==
E0110 15:12:22.127667       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0110 15:12:22.143245       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0110 15:12:22.187199       1 server_linux.go:66] "Using iptables proxy"
I0110 15:12:22.387448       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0110 15:12:22.387614       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0110 15:12:22.435084       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0110 15:12:22.435224       1 server_linux.go:169] "Using iptables Proxier"
I0110 15:12:22.440066       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0110 15:12:22.455381       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0110 15:12:22.471230       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0110 15:12:22.471626       1 server.go:483] "Version info" version="v1.31.0"
I0110 15:12:22.471730       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0110 15:12:22.475505       1 config.go:104] "Starting endpoint slice config controller"
I0110 15:12:22.475658       1 config.go:197] "Starting service config controller"
I0110 15:12:22.475704       1 shared_informer.go:313] Waiting for caches to sync for service config
I0110 15:12:22.475705       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0110 15:12:22.475770       1 config.go:326] "Starting node config controller"
I0110 15:12:22.475781       1 shared_informer.go:313] Waiting for caches to sync for node config
I0110 15:12:22.576283       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0110 15:12:22.576476       1 shared_informer.go:320] Caches are synced for service config
I0110 15:12:22.576527       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [2e40ed9a31b8] <==
E0110 15:12:14.063910       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0110 15:12:14.064268       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0110 15:12:14.064305       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0110 15:12:14.064330       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0110 15:12:14.064367       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.065155       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0110 15:12:14.065306       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.066394       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0110 15:12:14.066605       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0110 15:12:14.066681       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.066771       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0110 15:12:14.066463       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0110 15:12:14.066808       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0110 15:12:14.066847       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0110 15:12:14.066761       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.066509       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0110 15:12:14.066867       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0110 15:12:14.066992       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.066566       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0110 15:12:14.067267       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0110 15:12:14.067275       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.066677       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0110 15:12:14.067349       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.067343       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0110 15:12:14.067436       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.066676       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0110 15:12:14.067513       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.067567       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0110 15:12:14.067607       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.238536       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0110 15:12:14.238635       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.297833       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0110 15:12:14.297938       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:14.346294       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0110 15:12:14.346460       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0110 15:12:13.357337       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0110 15:12:13.357442       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.392565       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0110 15:12:13.392677       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.437983       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0110 15:12:13.438083       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.470642       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0110 15:12:13.470737       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.508783       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0110 15:12:13.508882       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.533787       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0110 15:12:13.533879       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.598027       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0110 15:12:13.598111       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.616563       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0110 15:12:13.616681       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.649699       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0110 15:12:13.649801       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.711941       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0110 15:12:13.712038       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.795264       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0110 15:12:13.795358       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0110 15:12:13.808557       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0110 15:12:13.808653       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0110 15:12:15.476187       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jan 10 15:12:16 minikube kubelet[2607]: I0110 15:12:16.933536    2607 apiserver.go:52] "Watching apiserver"
Jan 10 15:12:16 minikube kubelet[2607]: I0110 15:12:16.978005    2607 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Jan 10 15:12:17 minikube kubelet[2607]: I0110 15:12:17.483329    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=4.483294159 podStartE2EDuration="4.483294159s" podCreationTimestamp="2025-01-10 15:12:13 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-10 15:12:17.386472576 +0000 UTC m=+1.701353832" watchObservedRunningTime="2025-01-10 15:12:17.483294159 +0000 UTC m=+1.798175395"
Jan 10 15:12:17 minikube kubelet[2607]: I0110 15:12:17.507713    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.507688447 podStartE2EDuration="1.507688447s" podCreationTimestamp="2025-01-10 15:12:16 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-10 15:12:17.483695864 +0000 UTC m=+1.798577110" watchObservedRunningTime="2025-01-10 15:12:17.507688447 +0000 UTC m=+1.822569683"
Jan 10 15:12:17 minikube kubelet[2607]: I0110 15:12:17.589775    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.589739418 podStartE2EDuration="1.589739418s" podCreationTimestamp="2025-01-10 15:12:16 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-10 15:12:17.507638847 +0000 UTC m=+1.822520082" watchObservedRunningTime="2025-01-10 15:12:17.589739418 +0000 UTC m=+1.904620654"
Jan 10 15:12:17 minikube kubelet[2607]: I0110 15:12:17.590493    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.590448543 podStartE2EDuration="1.590448543s" podCreationTimestamp="2025-01-10 15:12:16 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-10 15:12:17.589107463 +0000 UTC m=+1.903988719" watchObservedRunningTime="2025-01-10 15:12:17.590448543 +0000 UTC m=+1.905329810"
Jan 10 15:12:20 minikube kubelet[2607]: I0110 15:12:20.091648    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/978f1e22-b1c8-4e87-b035-02401f6037be-tmp\") pod \"storage-provisioner\" (UID: \"978f1e22-b1c8-4e87-b035-02401f6037be\") " pod="kube-system/storage-provisioner"
Jan 10 15:12:20 minikube kubelet[2607]: I0110 15:12:20.091793    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-j72fz\" (UniqueName: \"kubernetes.io/projected/978f1e22-b1c8-4e87-b035-02401f6037be-kube-api-access-j72fz\") pod \"storage-provisioner\" (UID: \"978f1e22-b1c8-4e87-b035-02401f6037be\") " pod="kube-system/storage-provisioner"
Jan 10 15:12:20 minikube kubelet[2607]: E0110 15:12:20.206311    2607 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jan 10 15:12:20 minikube kubelet[2607]: E0110 15:12:20.206398    2607 projected.go:194] Error preparing data for projected volume kube-api-access-j72fz for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Jan 10 15:12:20 minikube kubelet[2607]: E0110 15:12:20.206544    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/978f1e22-b1c8-4e87-b035-02401f6037be-kube-api-access-j72fz podName:978f1e22-b1c8-4e87-b035-02401f6037be nodeName:}" failed. No retries permitted until 2025-01-10 15:12:20.706507452 +0000 UTC m=+5.021388687 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-j72fz" (UniqueName: "kubernetes.io/projected/978f1e22-b1c8-4e87-b035-02401f6037be-kube-api-access-j72fz") pod "storage-provisioner" (UID: "978f1e22-b1c8-4e87-b035-02401f6037be") : configmap "kube-root-ca.crt" not found
Jan 10 15:12:20 minikube kubelet[2607]: I0110 15:12:20.799363    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/babdd443-bbcd-42ac-8955-e7dd74817795-kube-proxy\") pod \"kube-proxy-r2kff\" (UID: \"babdd443-bbcd-42ac-8955-e7dd74817795\") " pod="kube-system/kube-proxy-r2kff"
Jan 10 15:12:20 minikube kubelet[2607]: I0110 15:12:20.799625    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/babdd443-bbcd-42ac-8955-e7dd74817795-lib-modules\") pod \"kube-proxy-r2kff\" (UID: \"babdd443-bbcd-42ac-8955-e7dd74817795\") " pod="kube-system/kube-proxy-r2kff"
Jan 10 15:12:20 minikube kubelet[2607]: I0110 15:12:20.799682    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/babdd443-bbcd-42ac-8955-e7dd74817795-xtables-lock\") pod \"kube-proxy-r2kff\" (UID: \"babdd443-bbcd-42ac-8955-e7dd74817795\") " pod="kube-system/kube-proxy-r2kff"
Jan 10 15:12:20 minikube kubelet[2607]: I0110 15:12:20.799752    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-c7pxv\" (UniqueName: \"kubernetes.io/projected/babdd443-bbcd-42ac-8955-e7dd74817795-kube-api-access-c7pxv\") pod \"kube-proxy-r2kff\" (UID: \"babdd443-bbcd-42ac-8955-e7dd74817795\") " pod="kube-system/kube-proxy-r2kff"
Jan 10 15:12:21 minikube kubelet[2607]: I0110 15:12:21.172256    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mjzd5\" (UniqueName: \"kubernetes.io/projected/0697bdfe-3387-4aad-be80-977654e91db7-kube-api-access-mjzd5\") pod \"coredns-6f6b679f8f-k7jlm\" (UID: \"0697bdfe-3387-4aad-be80-977654e91db7\") " pod="kube-system/coredns-6f6b679f8f-k7jlm"
Jan 10 15:12:21 minikube kubelet[2607]: I0110 15:12:21.172518    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/0697bdfe-3387-4aad-be80-977654e91db7-config-volume\") pod \"coredns-6f6b679f8f-k7jlm\" (UID: \"0697bdfe-3387-4aad-be80-977654e91db7\") " pod="kube-system/coredns-6f6b679f8f-k7jlm"
Jan 10 15:12:21 minikube kubelet[2607]: I0110 15:12:21.507867    2607 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5a46c5171600debf5c662f59da653e4ccd52062590be29d3c7227de05bc91122"
Jan 10 15:12:22 minikube kubelet[2607]: I0110 15:12:22.813805    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-r2kff" podStartSLOduration=2.8137608 podStartE2EDuration="2.8137608s" podCreationTimestamp="2025-01-10 15:12:20 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-10 15:12:22.787386591 +0000 UTC m=+7.102267858" watchObservedRunningTime="2025-01-10 15:12:22.8137608 +0000 UTC m=+7.128642056"
Jan 10 15:12:22 minikube kubelet[2607]: I0110 15:12:22.870623    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=4.870544902 podStartE2EDuration="4.870544902s" podCreationTimestamp="2025-01-10 15:12:18 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-10 15:12:22.813764011 +0000 UTC m=+7.128645278" watchObservedRunningTime="2025-01-10 15:12:22.870544902 +0000 UTC m=+7.185426148"
Jan 10 15:12:22 minikube kubelet[2607]: I0110 15:12:22.871077    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-6f6b679f8f-k7jlm" podStartSLOduration=2.871037678 podStartE2EDuration="2.871037678s" podCreationTimestamp="2025-01-10 15:12:20 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-01-10 15:12:22.870540898 +0000 UTC m=+7.185422154" watchObservedRunningTime="2025-01-10 15:12:22.871037678 +0000 UTC m=+7.185918934"
Jan 10 15:12:24 minikube kubelet[2607]: I0110 15:12:24.762833    2607 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Jan 10 15:12:26 minikube kubelet[2607]: I0110 15:12:26.880216    2607 kuberuntime_manager.go:1633] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jan 10 15:12:26 minikube kubelet[2607]: I0110 15:12:26.881810    2607 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jan 10 15:12:43 minikube kubelet[2607]: I0110 15:12:43.445459    2607 scope.go:117] "RemoveContainer" containerID="cf2938647d1caf1e58e1e15aac32985384d768325eefffebcddecbf150f6b7c6"
Jan 10 15:13:28 minikube kubelet[2607]: I0110 15:13:28.318561    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-686vs\" (UniqueName: \"kubernetes.io/projected/99637204-68de-4d31-9441-a21ccc1941c8-kube-api-access-686vs\") pod \"student-app-api-744bb4c9b8-zqsn5\" (UID: \"99637204-68de-4d31-9441-a21ccc1941c8\") " pod="default/student-app-api-744bb4c9b8-zqsn5"
Jan 10 15:13:28 minikube kubelet[2607]: I0110 15:13:28.520922    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b9h67\" (UniqueName: \"kubernetes.io/projected/d9875a9a-988c-45ab-abd9-98c0223316ab-kube-api-access-b9h67\") pod \"student-app-client-676874d64b-svh22\" (UID: \"d9875a9a-988c-45ab-abd9-98c0223316ab\") " pod="default/student-app-client-676874d64b-svh22"
Jan 10 15:13:29 minikube kubelet[2607]: I0110 15:13:29.070355    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-c9rl5\" (UniqueName: \"kubernetes.io/projected/7c5eae24-08f4-44b4-b320-318baf2132a0-kube-api-access-c9rl5\") pod \"mongo-5df4f657ff-vtgpv\" (UID: \"7c5eae24-08f4-44b4-b320-318baf2132a0\") " pod="default/mongo-5df4f657ff-vtgpv"
Jan 10 15:13:29 minikube kubelet[2607]: I0110 15:13:29.070549    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-d114fcc7-407a-4943-be4d-f86eed83e317\" (UniqueName: \"kubernetes.io/host-path/7c5eae24-08f4-44b4-b320-318baf2132a0-pvc-d114fcc7-407a-4943-be4d-f86eed83e317\") pod \"mongo-5df4f657ff-vtgpv\" (UID: \"7c5eae24-08f4-44b4-b320-318baf2132a0\") " pod="default/mongo-5df4f657ff-vtgpv"
Jan 10 15:14:07 minikube kubelet[2607]: I0110 15:14:07.584548    2607 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/student-app-api-744bb4c9b8-zqsn5" podStartSLOduration=-1.4499468549999999 podStartE2EDuration="39.584497851s" podCreationTimestamp="2025-01-10 15:13:28 +0000 UTC" firstStartedPulling="2025-01-10 15:13:29.284698046 +0000 UTC m=+76.800873672" lastFinishedPulling="2025-01-10 15:14:07.006780304 +0000 UTC m=+117.835318378" observedRunningTime="2025-01-10 15:14:07.583770846 +0000 UTC m=+118.412308941" watchObservedRunningTime="2025-01-10 15:14:07.584497851 +0000 UTC m=+118.413035967"
Jan 10 15:15:20 minikube kubelet[2607]: I0110 15:15:20.311590    2607 scope.go:117] "RemoveContainer" containerID="050705ef1a540dd00de778873673d229f3896ae5e8ee41efd43f793cdd2db5d4"
Jan 10 15:15:45 minikube kubelet[2607]: I0110 15:15:45.041803    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert\") pod \"ingress-nginx-controller-bc57996ff-gdmzx\" (UID: \"c5a2c4ea-2504-40f7-8bd6-abd522a8906d\") " pod="ingress-nginx/ingress-nginx-controller-bc57996ff-gdmzx"
Jan 10 15:15:45 minikube kubelet[2607]: I0110 15:15:45.042096    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tkv5w\" (UniqueName: \"kubernetes.io/projected/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-kube-api-access-tkv5w\") pod \"ingress-nginx-controller-bc57996ff-gdmzx\" (UID: \"c5a2c4ea-2504-40f7-8bd6-abd522a8906d\") " pod="ingress-nginx/ingress-nginx-controller-bc57996ff-gdmzx"
Jan 10 15:15:45 minikube kubelet[2607]: I0110 15:15:45.142580    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qzm2p\" (UniqueName: \"kubernetes.io/projected/881bdc3a-e70a-4439-b3d8-b0fe595b3b32-kube-api-access-qzm2p\") pod \"ingress-nginx-admission-create-xl5gm\" (UID: \"881bdc3a-e70a-4439-b3d8-b0fe595b3b32\") " pod="ingress-nginx/ingress-nginx-admission-create-xl5gm"
Jan 10 15:15:45 minikube kubelet[2607]: I0110 15:15:45.142800    2607 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-slrth\" (UniqueName: \"kubernetes.io/projected/1b0d1ddd-b1ee-4992-b298-a7f77a75f417-kube-api-access-slrth\") pod \"ingress-nginx-admission-patch-9pjlv\" (UID: \"1b0d1ddd-b1ee-4992-b298-a7f77a75f417\") " pod="ingress-nginx/ingress-nginx-admission-patch-9pjlv"
Jan 10 15:15:45 minikube kubelet[2607]: E0110 15:15:45.143205    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:15:45 minikube kubelet[2607]: E0110 15:15:45.143556    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:15:45.643440344 +0000 UTC m=+221.387403080 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:15:45 minikube kubelet[2607]: E0110 15:15:45.648022    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:15:45 minikube kubelet[2607]: E0110 15:15:45.648432    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:15:46.648229668 +0000 UTC m=+222.392192415 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:15:46 minikube kubelet[2607]: E0110 15:15:46.657245    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:15:46 minikube kubelet[2607]: E0110 15:15:46.657516    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:15:48.657476462 +0000 UTC m=+224.401439220 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:15:48 minikube kubelet[2607]: E0110 15:15:48.680122    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:15:48 minikube kubelet[2607]: E0110 15:15:48.680358    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:15:52.680322277 +0000 UTC m=+228.424285024 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:15:52 minikube kubelet[2607]: E0110 15:15:52.721038    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:15:52 minikube kubelet[2607]: E0110 15:15:52.721216    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:16:00.721182538 +0000 UTC m=+236.465145295 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:16:00 minikube kubelet[2607]: E0110 15:16:00.800750    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:16:00 minikube kubelet[2607]: E0110 15:16:00.801114    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:16:16.801069993 +0000 UTC m=+252.545032740 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:16:15 minikube kubelet[2607]: E0110 15:16:15.172803    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:16:15 minikube kubelet[2607]: E0110 15:16:15.173146    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:16:47.173091942 +0000 UTC m=+284.620262546 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:16:45 minikube kubelet[2607]: E0110 15:16:45.598946    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:16:45 minikube kubelet[2607]: E0110 15:16:45.599227    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:17:49.599182613 +0000 UTC m=+348.664885229 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:17:41 minikube kubelet[2607]: E0110 15:17:41.239087    2607 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-gdmzx" podUID="c5a2c4ea-2504-40f7-8bd6-abd522a8906d"
Jan 10 15:17:46 minikube kubelet[2607]: E0110 15:17:46.353717    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:17:46 minikube kubelet[2607]: E0110 15:17:46.353932    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:19:48.353890443 +0000 UTC m=+470.759673260 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:19:41 minikube kubelet[2607]: E0110 15:19:41.525746    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:19:41 minikube kubelet[2607]: E0110 15:19:41.526070    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:21:43.526033062 +0000 UTC m=+592.765491569 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:19:48 minikube kubelet[2607]: E0110 15:19:48.431491    2607 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-gdmzx" podUID="c5a2c4ea-2504-40f7-8bd6-abd522a8906d"
Jan 10 15:21:36 minikube kubelet[2607]: E0110 15:21:36.668752    2607 secret.go:188] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Jan 10 15:21:36 minikube kubelet[2607]: E0110 15:21:36.669113    2607 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert podName:c5a2c4ea-2504-40f7-8bd6-abd522a8906d nodeName:}" failed. No retries permitted until 2025-01-10 15:23:38.669074135 +0000 UTC m=+714.784686867 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/c5a2c4ea-2504-40f7-8bd6-abd522a8906d-webhook-cert") pod "ingress-nginx-controller-bc57996ff-gdmzx" (UID: "c5a2c4ea-2504-40f7-8bd6-abd522a8906d") : secret "ingress-nginx-admission" not found
Jan 10 15:21:59 minikube kubelet[2607]: E0110 15:21:59.595868    2607 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="ingress-nginx/ingress-nginx-controller-bc57996ff-gdmzx" podUID="c5a2c4ea-2504-40f7-8bd6-abd522a8906d"


==> storage-provisioner [cf2938647d1c] <==
I0110 15:12:21.682382       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0110 15:12:42.351260       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [da807a1b8079] <==
I0110 15:12:43.728719       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0110 15:12:43.749328       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0110 15:12:43.749579       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0110 15:12:43.773021       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0110 15:12:43.773429       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_cb673f7e-3bd1-4cd1-b51a-bae337ec0199!
I0110 15:12:43.773440       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a98e5449-e07e-4fff-a5ff-2b616005d0cb", APIVersion:"v1", ResourceVersion:"426", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_cb673f7e-3bd1-4cd1-b51a-bae337ec0199 became leader
I0110 15:12:43.875144       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_cb673f7e-3bd1-4cd1-b51a-bae337ec0199!
I0110 15:13:27.882674       1 controller.go:1332] provision "default/mongo-pvc" class "standard": started
I0110 15:13:27.882880       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    952659e4-a441-4d04-9102-72adc09a2974 322 0 2025-01-10 15:12:18 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2025-01-10 15:12:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-d114fcc7-407a-4943-be4d-f86eed83e317 &PersistentVolumeClaim{ObjectMeta:{mongo-pvc  default  d114fcc7-407a-4943-be4d-f86eed83e317 479 0 2025-01-10 15:13:27 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"mongo-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"256Mi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2025-01-10 15:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2025-01-10 15:13:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{268435456 0} {<nil>}  BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/mongo-pvc
I0110 15:13:27.886360       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc", UID:"d114fcc7-407a-4943-be4d-f86eed83e317", APIVersion:"v1", ResourceVersion:"479", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/mongo-pvc"
I0110 15:13:27.888066       1 controller.go:1439] provision "default/mongo-pvc" class "standard": volume "pvc-d114fcc7-407a-4943-be4d-f86eed83e317" provisioned
I0110 15:13:27.888676       1 controller.go:1456] provision "default/mongo-pvc" class "standard": succeeded
I0110 15:13:27.888933       1 volume_store.go:212] Trying to save persistentvolume "pvc-d114fcc7-407a-4943-be4d-f86eed83e317"
I0110 15:13:27.927318       1 volume_store.go:219] persistentvolume "pvc-d114fcc7-407a-4943-be4d-f86eed83e317" saved
I0110 15:13:27.968580       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"mongo-pvc", UID:"d114fcc7-407a-4943-be4d-f86eed83e317", APIVersion:"v1", ResourceVersion:"479", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-d114fcc7-407a-4943-be4d-f86eed83e317

